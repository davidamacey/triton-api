# End-to-End YOLO with Triton - Analysis & Implementation Plan

## Executive Summary

We can **eliminate ALL Python post-processing overhead** by baking NMS (Non-Maximum Suppression) directly into the ONNX/TensorRT model. This moves the entire YOLO pipeline onto the GPU:

- **Before:** Image â†’ TensorRT Inference â†’ CPU Python NMS â†’ Results
- **After:** Image â†’ TensorRT Inference+NMS â†’ Results

**Expected Performance Gain:** 2-5x faster inference by eliminating:
- GPUâ†’CPU memory transfer for raw detections (8400 boxes Ã— 84 values)
- Python NMS computation on CPU
- CPUâ†’GPU memory transfer (if needed)

---

## ğŸ” Key Findings from Reference Repos

### 1. **levipereira/ultralytics** - End2End Export Implementation

**Repository:** https://github.com/levipereira/ultralytics

**Key Innovation:** Custom `export_onnx_trt()` method that bakes TensorRT's EfficientNMS plugin into ONNX graph.

**How It Works:**
```python
from ultralytics import YOLO

# Load YOLOv11 model
model = YOLO("yolo11n.pt")

# Export with embedded NMS
model.export(
    format="onnx_trt",      # Special format with NMS
    topk_all=100,           # Max detections
    iou_thres=0.45,         # NMS IoU threshold
    conf_thres=0.25,        # Confidence threshold
    imgsz=640,
    dynamic=True,           # Dynamic batching
    simplify=True,
    half=True               # FP16
)
```

**Output Format Changes:**

| Standard ONNX | End2End ONNX |
|---------------|--------------|
| `output0: [84, 8400]` (raw detections) | `num_dets: [1]` (count)<br>`det_boxes: [TOPK, 4]` (boxes)<br>`det_scores: [TOPK]` (confidence)<br>`det_classes: [TOPK]` (class IDs) |
| Needs CPU NMS | NMS done on GPU! |

**Technical Implementation:**
- Uses `torch.autograd.Function` to inject `TRT::EfficientNMS_TRT` operator into ONNX graph
- TensorRT recognizes this custom operator and uses built-in EfficientNMS plugin
- Entire NMS operation compiled into TensorRT engine

**Triton Config for End2End Models:**
```protobuf
name: "yolov11_nano_end2end"
platform: "tensorrt_plan"
max_batch_size: 128

input [
  {
    name: "images"
    data_type: TYPE_FP32
    dims: [ 3, 640, 640 ]
  }
]

output [
  {
    name: "num_dets"
    data_type: TYPE_INT32
    dims: [ 1 ]
  },
  {
    name: "det_boxes"
    data_type: TYPE_FP32
    dims: [ 100, 4 ]  # TOPK=100
  },
  {
    name: "det_scores"
    data_type: TYPE_FP32
    dims: [ 100 ]
  },
  {
    name: "det_classes"
    data_type: TYPE_INT32
    dims: [ 100 ]
  }
]

dynamic_batching {
  preferred_batch_size: [ 8, 16, 32, 64, 128 ]
  max_queue_delay_microseconds: 100
}
```

---

### 2. **omarabid59/yolov8-triton** - Triton Ensemble Approach

**Repository:** https://github.com/omarabid59/yolov8-triton

**Approach:** Uses Triton ensemble to chain ONNX inference + Python postprocessing.

**Architecture:**
```
Ensemble Model
â”œâ”€â”€ ONNX Model (GPU inference)
â”‚   Input: images [1,3,640,640]
â”‚   Output: output0 [1,84,8400]
â”‚
â””â”€â”€ Postprocess Model (Python backend on CPU)
    Input: output0 [1,84,8400]
    Output: num_dets, det_boxes, det_scores, det_classes
```

**Limitation:** Python postprocessing runs on **CPU**, creating bottleneck:
- GPUâ†’CPU memory transfer overhead
- Python NMS slower than TensorRT NMS
- Cannot leverage full GPU parallelization

**Conclusion:** Good pattern for Triton ensembles, but CPU postprocessing defeats purpose.

---

### 3. **triton-server-yolo** - Production End2End Deployment

**Repository:** https://github.com/levipereira/triton-server-yolo

**Highlights:**
- Demonstrates YOLOv7, YOLOv8, YOLOv9 end2end deployment on Triton
- Benchmark results showing 400+ infer/sec on RTX 4090
- Two model types:
  - **Inference models:** `--topk-all 100 --iou-thres 0.45 --conf-thres 0.25`
  - **Evaluation models:** `--topk-all 300 --iou-thres 0.7 --conf-thres 0.001`

**Key Parameters:**
```bash
# Export YOLOv8 with end2end NMS
python export.py \
  --weights yolov8n.pt \
  --topk-all 100 \
  --iou-thres 0.45 \
  --conf-thres 0.25 \
  --include onnx_end2end
```

---

## ğŸ“Š Performance Comparison

### Current Architecture (Standard ONNX)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI    â”‚â”€â”€â”€â–¶â”‚  Triton+TensorRT  â”‚â”€â”€â”€â–¶â”‚ GPUâ†’CPU copy â”‚
â”‚  (receives   â”‚    â”‚  (inference only) â”‚    â”‚ 8400Ã—84 vals â”‚
â”‚   image)     â”‚    â”‚   [84, 8400]      â”‚    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                     â”‚
                                                     â–¼
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                            â”‚ Python NMS   â”‚
                                            â”‚  (CPU only)  â”‚
                                            â”‚   ~5-10ms    â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
**Bottlenecks:**
- GPUâ†’CPU memory transfer: ~1-2ms per request
- Python NMS on CPU: ~5-10ms per request
- Cannot batch NMS across concurrent requests

---

### Proposed Architecture (End2End ONNX)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI    â”‚â”€â”€â”€â–¶â”‚  Triton+TensorRT+EfficientNMS  â”‚
â”‚  (receives   â”‚    â”‚  (all-GPU pipeline)            â”‚
â”‚   image)     â”‚    â”‚  âœ“ Inference                   â”‚
â”‚              â”‚    â”‚  âœ“ NMS                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  âœ“ Top-K selection             â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Return boxes â”‚
                        â”‚  (100 max)   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
**Benefits:**
- **Zero GPUâ†’CPU transfer** for raw detections
- **Hardware-accelerated NMS** on GPU (~0.5ms)
- **Batched NMS** - process multiple requests simultaneously
- **Lower latency** - no CPU wait time

---

## ğŸ¯ Implementation Plan

### Phase 1: Setup Custom Ultralytics Fork
```bash
# 1. Install levipereira's ultralytics fork in pytorch-api container
docker compose exec pytorch-api pip uninstall ultralytics -y
docker compose exec pytorch-api pip install git+https://github.com/levipereira/ultralytics.git

# 2. Verify onnx_trt export format is available
docker compose exec pytorch-api python -c "from ultralytics import YOLO; help(YOLO.export)"
```

---

### Phase 2: Export End2End Models

Create `scripts/export_end2end.py`:

```python
#!/usr/bin/env python3
"""
Export YOLO11 models in End2End format with embedded NMS for Triton.

This creates TensorRT-optimized ONNX files with EfficientNMS plugin baked in.
No post-processing needed - inference returns final detections directly.
"""
from pathlib import Path
from ultralytics import YOLO

# Model configurations for RTX A6000
MODELS = {
    "yolo11n": {
        "pt_file": "/app/pytorch_models/yolo11n.pt",
        "triton_name": "yolov11_nano_end2end",
        "max_batch": 128,
        "topk": 100  # Max detections per image
    },
    "yolo11s": {
        "pt_file": "/app/pytorch_models/yolo11s.pt",
        "triton_name": "yolov11_small_end2end",
        "max_batch": 64,
        "topk": 100
    },
    "yolo11m": {
        "pt_file": "/app/pytorch_models/yolo11m.pt",
        "triton_name": "yolov11_medium_end2end",
        "max_batch": 32,
        "topk": 100
    },
}

# NMS parameters (tunable)
IOU_THRESHOLD = 0.45  # NMS IoU threshold
CONF_THRESHOLD = 0.25  # Confidence threshold
IMG_SIZE = 640

for model_id, config in MODELS.items():
    print(f"\n{'='*80}")
    print(f"Exporting {model_id} â†’ {config['triton_name']}")
    print(f"{'='*80}")

    pt_file = config["pt_file"]
    triton_name = config["triton_name"]

    if not Path(pt_file).exists():
        print(f"âœ— Model file not found: {pt_file}")
        continue

    model = YOLO(pt_file)

    # Export with embedded NMS
    print(f"  Exporting with EfficientNMS plugin...")
    print(f"  - topk_all={config['topk']}")
    print(f"  - iou_thres={IOU_THRESHOLD}")
    print(f"  - conf_thres={CONF_THRESHOLD}")
    print(f"  - dynamic=True (variable batch size)")
    print(f"  - half=True (FP16 precision)")

    export_path = model.export(
        format="onnx_trt",           # End2End format with NMS
        imgsz=IMG_SIZE,
        topk_all=config['topk'],     # Max detections
        iou_thres=IOU_THRESHOLD,     # NMS IoU threshold
        conf_thres=CONF_THRESHOLD,   # Confidence threshold
        dynamic=True,                # Dynamic batching support
        simplify=True,               # Optimize graph
        half=True,                   # FP16
        device=0                     # GPU 0
    )

    # Move to Triton model repository
    onnx_model_dir = Path(f"/app/models/{triton_name}/1")
    onnx_model_dir.mkdir(parents=True, exist_ok=True)

    onnx_dest = onnx_model_dir / "model.onnx"

    # The export creates {model_name}-trt.onnx
    export_file = Path(export_path)
    if export_file.exists():
        import shutil
        shutil.copy2(export_file, onnx_dest)
        print(f"  âœ“ Exported to: {onnx_dest}")
    else:
        print(f"  âœ— Export failed, file not found: {export_file}")
```

**Run export:**
```bash
docker compose exec pytorch-api python /app/scripts/export_end2end.py
```

---

### Phase 3: Create Triton Configs for End2End Models

Create `models/yolov11_nano_end2end/config.pbtxt`:

```protobuf
# YOLO11 Nano - End2End with EfficientNMS
# No post-processing needed - returns final detections

name: "yolov11_nano_end2end"
platform: "onnxruntime_onnx"  # Or "tensorrt_plan" after converting
max_batch_size: 128

input [
  {
    name: "images"
    data_type: TYPE_FP32
    dims: [ 3, 640, 640 ]
  }
]

output [
  {
    name: "num_dets"
    data_type: TYPE_INT32
    dims: [ 1 ]
  },
  {
    name: "det_boxes"
    data_type: TYPE_FP32
    dims: [ 100, 4 ]
  },
  {
    name: "det_scores"
    data_type: TYPE_FP32
    dims: [ 100 ]
  },
  {
    name: "det_classes"
    data_type: TYPE_INT32
    dims: [ 100 ]
  }
]

instance_group [
  {
    count: 2
    kind: KIND_GPU
    gpus: [ 0 ]
  }
]

dynamic_batching {
  preferred_batch_size: [ 8, 16, 32, 64, 128 ]
  max_queue_delay_microseconds: 100
  preserve_ordering: false

  default_queue_policy {
    timeout_action: REJECT
    default_timeout_microseconds: 1000000
    allow_timeout_override: false
    max_queue_size: 256
  }
}

optimization {
  execution_accelerators {
    gpu_execution_accelerator {
      name: "tensorrt"
      parameters { key: "precision_mode", value: "FP16" }
      parameters { key: "max_workspace_size_bytes", value: "4294967296" }
      parameters { key: "trt_engine_cache_enable", value: "1" }
      parameters { key: "trt_engine_cache_path", value: "/trt_cache/yolov11_nano_end2end" }
    }
  }
}

model_warmup [
  { name: "warmup_1", batch_size: 1, count: 3,
    inputs { key: "images" value { data_type: TYPE_FP32, dims: [3,640,640], random_data: true }}},
  { name: "warmup_64", batch_size: 64, count: 1,
    inputs { key: "images" value { data_type: TYPE_FP32, dims: [3,640,640], random_data: true }}},
  { name: "warmup_128", batch_size: 128, count: 1,
    inputs { key: "images" value { data_type: TYPE_FP32, dims: [3,640,640], random_data: true }}}
]
```

Repeat for `yolov11_small_end2end` (max_batch=64) and `yolov11_medium_end2end` (max_batch=32).

---

### Phase 4: Update FastAPI Client

Update `src/main.py` to handle new output format:

```python
@app.post("/predict/{model_name}")
async def predict(model_name: str, file: UploadFile = File(...)):
    """
    Predict objects in image using end2end YOLO model.

    End2end models return final detections directly (no NMS needed).
    """
    # Validate model
    model_map = {
        "nano": "yolov11_nano_end2end",
        "small": "yolov11_small_end2end",
        "medium": "yolov11_medium_end2end"
    }

    triton_model = model_map.get(model_name)
    if not triton_model:
        raise HTTPException(status_code=400, detail=f"Invalid model: {model_name}")

    # Read and preprocess image
    contents = await file.read()
    img = Image.open(io.BytesIO(contents)).convert("RGB")
    img_array = np.array(img).astype(np.float32) / 255.0
    img_array = np.transpose(img_array, (2, 0, 1))  # HWC â†’ CHW
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension

    # Prepare Triton inference request
    inputs = [
        httpclient.InferInput("images", img_array.shape, "FP32")
    ]
    inputs[0].set_data_from_numpy(img_array)

    outputs = [
        httpclient.InferRequestedOutput("num_dets"),
        httpclient.InferRequestedOutput("det_boxes"),
        httpclient.InferRequestedOutput("det_scores"),
        httpclient.InferRequestedOutput("det_classes")
    ]

    # Run inference (async, non-blocking)
    triton_client = httpclient.InferenceServerClient(url="triton-api:8000")

    response = await asyncio.to_thread(
        triton_client.infer,
        model_name=triton_model,
        inputs=inputs,
        outputs=outputs
    )

    # Extract results (already post-processed!)
    num_dets = response.as_numpy("num_dets")[0]
    det_boxes = response.as_numpy("det_boxes")[:num_dets]  # [N, 4]
    det_scores = response.as_numpy("det_scores")[:num_dets]  # [N]
    det_classes = response.as_numpy("det_classes")[:num_dets]  # [N]

    # Format response
    detections = []
    for i in range(num_dets):
        x, y, w, h = det_boxes[i]
        detections.append({
            "x1": float(x - w/2),
            "y1": float(y - h/2),
            "x2": float(x + w/2),
            "y2": float(y + h/2),
            "confidence": float(det_scores[i]),
            "class": int(det_classes[i])
        })

    return {
        "detections": detections,
        "num_detections": int(num_dets),
        "status": "success"
    }
```

**Key changes:**
- Request 4 outputs instead of 1
- Extract `num_dets` to know how many valid detections
- No NMS needed - already done on GPU!

---

### Phase 5: Update docker-compose.yml

```yaml
services:
  triton-api:
    command:
      - tritonserver
      - --model-store=/models
      - --backend-config=default-max-batch-size=128
      - --strict-model-config=false
      - --model-control-mode=explicit
      - --load-model=yolov11_nano_end2end
      - --load-model=yolov11_small_end2end
      - --load-model=yolov11_medium_end2end
      - --log-verbose=1
```

---

### Phase 6: Testing & Benchmarking

**Test 1: Verify End2End Models Load**
```bash
docker compose restart triton-api
docker compose logs triton-api | grep "READY"

# Expected output:
# yolov11_nano_end2end | 1 | READY
# yolov11_small_end2end | 1 | READY
# yolov11_medium_end2end | 1 | READY
```

**Test 2: Single Inference**
```bash
curl -X POST http://localhost:9600/predict/nano \
  -F "file=@test_image.jpg"
```

**Test 3: Benchmark Comparison**

Run `perf_analyzer` to compare:
- **Track A (Old):** TensorRT + CPU Python NMS
- **Track A+ (New):** TensorRT + GPU EfficientNMS (end2end)
- **Track B:** PyTorch

```bash
# Benchmark end2end model
docker compose run triton-sdk perf_analyzer \
  -m yolov11_nano_end2end \
  -u triton-api:8001 \
  -i grpc \
  --shared-memory system \
  --concurrency-range 8:128:8
```

---

## ğŸš€ Expected Performance Gains

Based on triton-server-yolo benchmarks (RTX 4090):

| Metric | Standard ONNX | End2End ONNX | Improvement |
|--------|--------------|--------------|-------------|
| Latency (batch=1) | ~3.5ms | ~2.4ms | **31% faster** |
| Throughput (batch=8) | ~250 inf/sec | ~418 inf/sec | **67% faster** |
| GPU Utilization | 60-70% | 85-95% | **Better GPU usage** |
| CPU Usage | High (NMS) | Minimal | **95% less CPU** |

On our RTX A6000 (48GB, more powerful), expect:
- **Latency:** 1.5-2ms per inference (batch=1)
- **Throughput:** 500-700 infer/sec (batch=64+)
- **Concurrency:** Handle 100+ concurrent requests with dynamic batching

---

## ğŸ“ Summary

### Current State
- âœ… TensorRT inference working
- âŒ CPU post-processing bottleneck
- âŒ GPUâ†’CPU memory transfer overhead

### After Implementation
- âœ… End-to-end GPU pipeline (preprocessing can be added later)
- âœ… Hardware-accelerated NMS via EfficientNMS plugin
- âœ… Zero GPUâ†’CPU transfers for detections
- âœ… Batched NMS for concurrent requests

### Next Steps
1. Install levipereira/ultralytics fork
2. Export models with `format="onnx_trt"`
3. Create end2end Triton configs
4. Update FastAPI to handle new output format
5. Benchmark and compare

---

## ğŸ”— References

- **levipereira/ultralytics:** https://github.com/levipereira/ultralytics
- **triton-server-yolo:** https://github.com/levipereira/triton-server-yolo
- **TensorRT EfficientNMS:** https://github.com/NVIDIA/TensorRT/tree/main/plugin/efficientNMSPlugin
- **Triton Ensembles:** https://github.com/triton-inference-server/server/blob/main/docs/user_guide/architecture.md#ensemble-models

---

**Generated:** 2025-11-15
**Author:** Claude Code Analysis
