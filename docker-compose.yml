services:
  triton-api:
    build:
      context: .
      dockerfile: Dockerfile.triton
    image: triton-api-pytorch:25.10
    container_name: triton-api
    restart: always
    ports:
      - 4600:8000  # HTTP
      - 4601:8001  # gRPC
      - 4602:8002  # Metrics
    volumes:
      - ./models:/models
    command:
      - tritonserver
      - --model-store=/models
      - --backend-config=default-max-batch-size=128
      - --strict-model-config=false
      - --model-control-mode=explicit
      # ===================================================================
      # TRACK E ONLY CONFIGURATION (Visual Search + Face Detection)
      # ===================================================================
      # Optimized for Track E with maximum GPU memory for face models
      # GPU 0: Track E visual search (YOLO + MobileCLIP)
      # ===================================================================
      #
      # Track E Core: YOLO Detection + MobileCLIP Embeddings
      - --load-model=yolov11_small_trt_end2end         # YOLO detection (TRT + GPU NMS)
      - --load-model=yolo_clip_preprocess_dali         # Fast DALI (YOLO 640 + CLIP 256)
      - --load-model=dual_preprocess_dali              # Triple-branch DALI (includes HD crop)
      - --load-model=mobileclip2_s2_image_encoder      # Image embeddings (TensorRT)
      - --load-model=mobileclip2_s2_text_encoder       # Text embeddings (TensorRT)
      - --load-model=box_embedding_extractor           # Per-box embeddings (Python + BLS)
      - --load-model=yolo_clip_ensemble                # Track E basic (detection + global embed)
      - --load-model=yolo_mobileclip_ensemble          # Track E full (+ per-box embeddings)
      #
      # Track E Face Detection & Recognition
      - --load-model=quad_preprocess_dali             # Quad-branch DALI (YOLO + CLIP + SCRFD + HD)
      - --load-model=scrfd_10g_face_detect            # Face detection (TensorRT)
      - --load-model=arcface_w600k_r50                # Face recognition (TensorRT)
      - --load-model=face_pipeline                    # Face pipeline (Python backend)
      - --load-model=yolo_face_clip_ensemble          # Unified ensemble (YOLO + CLIP + SCRFD + ArcFace)
      - --load-model=unified_embedding_extractor      # Unified extractor (box + face)
      - --load-model=yolo_unified_ensemble            # Efficient pipeline (face detection on person crops only)
      # ===================================================================
      - --log-verbose=1
      - --log-info=true
    shm_size: 8g
    ulimits:
      memlock: -1
      stack: 67108864
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]  # GPU 0: Track E only (face models to be added)
              capabilities:
                - gpu
    networks:
      - triton_net

  yolo-api:
    container_name: yolo-api
    pull_policy: always
    restart: always
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - ./src:/app/src
      - ./export:/app/export
      - ./dali:/app/dali
      - ./scripts:/app/scripts
      - ./tests:/app/tests
      - ./benchmarks:/app/benchmarks
      - ./models:/app/models
      - ./pytorch_models:/app/pytorch_models
      - ./test_images:/app/test_images
      - ./reference_repos:/app/reference_repos  # Track E: MobileCLIP repos (persisted)
      - ./cache/huggingface:/home/appuser/.cache/huggingface  # Track E: HuggingFace tokenizers (appuser)
      - ./outputs:/app/outputs  # Track E: Mosaic outputs and clustering results
      - /mnt/nvm/KILLBOY_SAMPLE_PICTURES:/mnt/nvm/KILLBOY_SAMPLE_PICTURES:ro  # Test data (YOLO)
      - /mnt/nvm/FACE_TEST_IMAGES:/mnt/nvm/FACE_TEST_IMAGES:ro  # Test data (faces)
    environment:
      - ENABLE_PYTORCH=false  # Track A disabled - Track E only mode
      # HuggingFace cache configuration (appuser home directory)
      - HF_HOME=/home/appuser/.cache/huggingface
      - HF_HUB_CACHE=/home/appuser/.cache/huggingface/hub
    shm_size: 8g
    stdin_open: true # Keep stdin open
    tty: true # Allocate a pseudo-TTY
    command:
      - uvicorn
      - src.main:app
      - --host=0.0.0.0
      - --port=8000
      # ===================================================================
      # Worker Configuration (Performance Tuning)
      # Production: 64 workers for dual-GPU throughput
      # Benchmark: 4 workers to match TRT instance count for fair comparison
      # ===================================================================
      - --workers=4  # Match TRT instance count (4) for fair benchmark comparison
      - --limit-max-requests=10000       # Recycle workers every 10k requests (prevents memory leaks)
      # ===================================================================
      # Event Loop & HTTP Protocol (Already Optimized)
      # ===================================================================
      - --loop=uvloop                    # 2-3x faster than asyncio default loop
      - --http=httptools                 # Faster HTTP parsing than h11
      # ===================================================================
      # Connection & Concurrency Settings
      # ===================================================================
      - --backlog=4096                   # Socket connection queue size
      - --limit-concurrency=512          # Max concurrent requests (prevents overload)
      - --timeout-keep-alive=75          # Keep connections alive for reuse
      - --timeout-graceful-shutdown=30   # Clean shutdown (drain connections)
      # ===================================================================
      # Logging
      # ===================================================================
      - --access-log
      - --log-level=info
    ports:
      - 4603:8000  # FastAPI
    ulimits:
      memlock: -1
      stack: 67108864
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities:
                - gpu
    depends_on:
      - triton-api
    networks:
      - triton_net

  triton-sdk:
    image: nvcr.io/nvidia/tritonserver:25.10-py3-sdk
    container_name: triton-sdk
    profiles:
      - benchmark
    command: sleep infinity
    volumes:
      - ./benchmarks:/workspace/benchmarks
      - ./models:/models
    networks:
      - triton_net

  node-exporter:
    image: prom/node-exporter:latest
    container_name: triton-node-exporter
    restart: always
    command:
      - '--path.rootfs=/host'
    pid: host
    volumes:
      - '/:/host:ro,rslave'
    networks:
      - triton_net

  prometheus:
    image: prom/prometheus:latest
    container_name: triton-prometheus
    restart: always
    ports:
      - 4604:9090  # Prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/alerts:/etc/prometheus/alerts
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    networks:
      - triton_net
    depends_on:
      - triton-api
      - node-exporter

  grafana:
    image: grafana/grafana:latest
    container_name: triton-grafana
    restart: always
    ports:
      - 4605:3000  # Grafana
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
      - ./monitoring/grafana-dashboards.yml:/etc/grafana/provisioning/dashboards/dashboards.yml
      - ./monitoring/dashboards:/etc/grafana/dashboards
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    networks:
      - triton_net
    depends_on:
      - prometheus
      - loki

  loki:
    image: grafana/loki:latest
    container_name: triton-loki
    restart: always
    user: "0"  # Run as root to avoid permission issues
    ports:
      - 4606:3100  # Loki
    volumes:
      - ./monitoring/loki-config.yml:/etc/loki/local-config.yaml
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - triton_net

  promtail:
    image: grafana/promtail:latest
    container_name: triton-promtail
    restart: always
    volumes:
      - ./monitoring/promtail-config.yml:/etc/promtail/config.yml
      - /var/run/docker.sock:/var/run/docker.sock
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    networks:
      - triton_net
    depends_on:
      - loki

  # ===================================================================
  # Track E: OpenSearch for Visual Search (Vector Database)
  # OpenSearch 3.x with k-NN plugin for vector similarity search
  # ===================================================================
  opensearch:
    image: opensearchproject/opensearch:3.3.1
    container_name: triton-opensearch
    restart: always
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms2g -Xmx2g"  # Heap size (adjust based on RAM)
      - DISABLE_SECURITY_PLUGIN=true  # Disable security for dev (enable in production)
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - opensearch_data:/usr/share/opensearch/data
    ports:
      - 4607:9200  # REST API
    healthcheck:
      test: ["CMD-SHELL", "curl -sS http://localhost:9200 || exit 1"]
      interval: 5s
      timeout: 10s
      retries: 20
    networks:
      - triton_net

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:3.3.0
    container_name: triton-opensearch-dashboards
    restart: always
    environment:
      - OPENSEARCH_HOSTS=["http://opensearch:9200"]
      - DISABLE_SECURITY_DASHBOARDS_PLUGIN=true  # Must match OpenSearch security setting
    ports:
      - 4608:5601  # OpenSearch Dashboards
    volumes:
      - opensearch_dashboards_data:/usr/share/opensearch-dashboards/data
    healthcheck:
      test: ["CMD-SHELL", "curl -sS http://localhost:5601 || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 20
    networks:
      - triton_net
    depends_on:
      opensearch:
        condition: service_healthy

networks:
  triton_net:
    driver: bridge

volumes:
  prometheus_data:
  grafana_data:
  loki_data:
  opensearch_data:
  opensearch_dashboards_data:
